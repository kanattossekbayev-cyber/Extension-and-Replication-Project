# PART 0: LOAD NECESSARY LIBRARIES
# -------------------------------------------------------------------
# This line installs all required packages for the tutorial.
# Note: You only need to run 'install.packages' once.
# After that, you can comment it out.
install.packages(c("igraph", "sna", "Matrix", "ergm", "network", "latentnet",
"ggplot2", "dplyr", "tidyr", "tibble"))
# Load packages for the session
library(igraph)    # For network analysis and visualization (Parts 1, 2, 3)
library(sna)       # For specific centrality measures (Part 1) and ERGM (Part 3)
# -------------------------------------------------------------------
# PART 1: DESCRIPTIVE NETWORK ANALYSIS
# -------------------------------------------------------------------
# This section demonstrates creating a network from a matrix and
# calculating key descriptive statistics.
## 1.1: Manually Construct the Adjacency Matrix
# Replicates the 5-node international relations network from
# Hafner-Burton, Kahler, and Montgomery (2009), Figure 1.
# The values represent the strength of the relationship.
hafner_burton_matrix <- matrix(
# Data: entered row-by-row
c(4, 4, 1, 0, 0,  # US
4, 5, 2, 0, 0,  # France
1, 2, 4, 1, 1,  # China
0, 0, 1, 1, 0,  # North Korea
0, 0, 1, 0, 1), # Iran
nrow = 5,
ncol = 5,
byrow = TRUE,
# Assign node names (countries)
dimnames = list(c("United States", "France", "China", "North Korea", "Iran"),
c("United States", "France", "China", "North Korea", "Iran"))
)
print(hafner_burton_matrix)
## 1.2: Create igraph Object and Plot
# Convert the matrix into an igraph network object.
# igraph is a powerful package for network analysis and visualization.
g_hb <- graph_from_adjacency_matrix(
hafner_burton_matrix,
mode = "undirected", # The matrix is symmetric, so the relationship is undirected
weighted = TRUE,     # The ties have values (strengths), so it's weighted
diag = FALSE         # Ignore the diagonal values (self-loops)
)
# 'UNW-' confirms: 'U' (Undirected), 'N' (Named), 'W' (Weighted)
print(g_hb)
# Plot the graph
# We use the edge 'weight' attribute to set the edge thickness.
# This visually represents the strength of the relationship.
plot(g_hb,
edge.width = E(g_hb)$weight,  # Thicker lines for stronger ties
vertex.size = 30,
vertex.label.cex = 1.3,
main = "Hafner-Burton et al. Network (Weights as Thickness)")
## 1.3: Calculate Centrality Measures (igraph)
# Centrality measures help us understand the importance of different nodes.
# Degree Centrality (Weighted)
# For weighted networks, 'strength()' is the correct function.
# It sums the weights of all ties connected to a node, not just the count.
degree_cent <- strength(g_hb)
print(degree_cent)
# Degree Centrality (Unweighted)
# This is the simple count of neighbors, ignoring weights.
# We use 'igraph::degree' to specify the function from the 'igraph'
# package, avoiding a conflict with 'sna::degree'.
degree_cent_unweighted <- igraph::degree(g_hb)
print(degree_cent_unweighted)
# Eigenvector Centrality
# Measures a node's importance based on the importance of its neighbors.
# A high score means a node is connected to other important nodes.
cent_eigen <- igraph::eigen_centrality(g_hb)$vector
print(cent_eigen)
# Closeness & Betweenness Centrality
# These are based on "shortest paths". In a weighted network, the shortest
# path is the one with the lowest sum of edge weights.
# Our weights represent strength, so a strong tie (5) is a short distance.
# We must invert the weights to create a 'distance' attribute.
E(g_hb)$distance <- 1 / E(g_hb)$weight
# Closeness Centrality
# Measures how quickly a node can reach all other nodes (average shortest path).
# We use our new 'distance' attribute as the weights.
cent_closeness <- igraph::closeness(g_hb, weights = E(g_hb)$distance, mode = "all")
print(cent_closeness)
# Betweenness Centrality
# Measures how often a node lies on the shortest path between two other nodes.
# A high score suggests a "broker" or "bridge" role.
cent_betweenness <- igraph::betweenness(g_hb, weights = E(g_hb)$distance, directed = FALSE)
print(cent_betweenness)
## 1.4: Calculate Centrality Measures (sna)
# The 'sna' package offers some measures not in igraph.
# 'sna' functions require a standard matrix, not an igraph object.
sparse_matrix <- as_adjacency_matrix(g_hb, attr = "weight")
g_hb_matrix <- as.matrix(sparse_matrix)
# Flow Betweenness Centrality
# A variation of betweenness that considers all paths, not just shortest.
cent_flow_betweenness <- flowbet(g_hb_matrix, gmode = "graph")
# Information Centrality
# Measures centrality based on the flow of information in the network.
cent_information <- infocent(g_hb_matrix, gmode = "graph")
## 1.5: Calculate Global Network Measures
# These measures describe the network as a whole.
# Network Density: The ratio of actual ties to all possible ties.
# A density of 1 means all nodes are connected; 0 means no connections.
print(edge_density(g_hb)) # Ignores weights by default
# Network Diameter: The "longest shortest path" in the network.
print(igraph::diameter(g_hb, weights = E(g_hb)$distance))
# -------------------------------------------------------------------
# PART 2: UNCOVERING NETWORK STRUCTURE (COMMUNITY DETECTION)
# -------------------------------------------------------------------
# This section demonstrates how to find "communities" or "clusters"
# in a network.
## 2.1: Create a Sample Graph with Known Clusters
# We'll create a graph that has a known structure, then see if the
# algorithm can find it.
# Set a seed for reproducible random results
seed <- 1234567
set.seed(seed)
# Create a graph with 3 'islands' (clusters) of 4 nodes each
g_12 <- sample_islands(
islands.n = 3,       # We want 3 clusters
islands.size = 4,    # Each cluster has 4 nodes
islands.pin = 0.7,   # 70% chance of an edge within a cluster (high)
n.inter = 1          # Only 1 edge between clusters (low)
)
# Assign names to the nodes
V(g_12)$name <- c("Quinn", "Bruce", "Songtao", "Cihan", "Maya", "Yunfan",
"Christine", "Yasin", "Viktar", "Kanat", "Jiahui", "Lesley")
# Plot the graph (note the clear cluster structure)
plot(g_12, vertex.size = 30, vertex.label.cex = 1.3,
main = "Sample Graph (3 Known Clusters)")
PART 3: INFERENTIAL NETWORK MODELING (ERGM & LATENTNET)
setwd("C:/Users/Kanat/Desktop")
install.packages(c("readr", "dplyr", "stringr", "tidytext"))
comments <- read_csv("cms_comments.csv")
install.packages("readr")
library(readr)
comments <- read_csv("cms_comments.csv")
library(readr)
comments <- read_csv(file.choose())
names(comments)
head(comments, 3)
library(readr)
library(dplyr)
library(stringr)
library(tidytext)
comments_clean <- comments %>%
select(
comment_id   = `Document ID`,   # колонка с ID
comment_text = Comment          # колонка с текстом комментария
) %>%
filter(!is.na(comment_text), comment_text != "")
head(comments_clean, 3)
head(comments_clean$comment_text, 5)
# Разбиваем текст комментариев на отдельные слова
tokens <- comments_clean %>%
unnest_tokens(word, comment_text)
# Загружаем лексикон Bing (positive/negative слова)
bing_lex <- get_sentiments("bing")
# Считаем позитивные/негативные слова по каждому комменту
sentiment_scores <- tokens %>%
inner_join(bing_lex, by = "word") %>%
count(comment_id, sentiment) %>%
tidyr::pivot_wider(
names_from  = sentiment,
values_from = n,
values_fill = 0
) %>%
mutate(sentiment_score = positive - negative)
# Смотрим результаты
sentiment_scores
sentiment_scores
comments <- read_csv("cms_comments.csv")
summary(sentiment_scores$sentiment_score)
library(ggplot2)
ggplot(sentiment_scores, aes(x = sentiment_score)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
theme_minimal() +
labs(
title = "Distribution of Sentiment Scores for CMS-2023-0144 Comments",
x = "Sentiment Score (positive - negative)",
y = "Count of Comments"
)
Сохранить как PNG
ggsave(
filename = "sentiment_histogram.png",
plot     = p,
width    = 8,
height   = 5,
dpi      = 300
)
ggsave(
filename = "C:/Users/User/Desktop/sentiment_histogram.png",
plot     = p,
width    = 8,
height   = 5,
dpi      = 300
)
p <- ggplot(...)
library(ggplot2)
p <- ggplot(sentiment_scores, aes(x = sentiment_score)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
theme_minimal() +
labs(
title = "Distribution of Sentiment Scores for CMS-2023-0144 Comments",
x = "Sentiment Score (positive - negative)",
y = "Count of Comments"
)
p
ggsave(
filename = "C:/Users/User/Desktop/sentiment_histogram.png",
plot     = p,
width    = 8,
height   = 5,
dpi      = 300
)
library(dplyr)
top20_positive <- sentiment_scores %>%
arrange(desc(sentiment_score)) %>%     # сортируем по убыванию
slice_head(n = 20) %>%                 # берём первые 20
left_join(comments_clean, by = "comment_id")   # добавляем текст
# посмотреть в RStudio
View(top20_positive)
# сохранить в CSV (опционально)
write.csv(top20_positive, "top20_positive_comments.csv", row.names = FALSE)
top20_negative <- sentiment_scores %>%
arrange(sentiment_score) %>%           # сортируем по возрастанию (от -∞ к +∞)
slice_head(n = 20) %>%                 # первые 20 = самые негативные
left_join(comments_clean, by = "comment_id")   # добавляем текст
# посмотреть в RStudio
View(top20_negative)
# сохранить в CSV (опционально)
write.csv(top20_negative, "top20_negative_comments.csv", row.names = FALSE)
library(dplyr)
sentiment_scores_grouped <- sentiment_scores %>%
mutate(
sentiment_group = case_when(
sentiment_score > 0  ~ "Positive",
sentiment_score < 0  ~ "Negative",
TRUE                 ~ "Neutral"
)
)
table(sentiment_scores_grouped$sentiment_group)
comparison_table <- sentiment_scores_grouped %>%
group_by(sentiment_group) %>%
summarise(
n           = n(),
perc        = n() / nrow(sentiment_scores_grouped) * 100,
mean_score  = mean(sentiment_score),
median_score= median(sentiment_score),
min_score   = min(sentiment_score),
max_score   = max(sentiment_score),
.groups = "drop"
)
comparison_table
comparison_pos_neg <- sentiment_scores_grouped %>%
filter(sentiment_group != "Neutral") %>%
group_by(sentiment_group) %>%
summarise(
n           = n(),
perc        = n() / nrow(sentiment_scores_grouped) * 100,
mean_score  = mean(sentiment_score),
median_score= median(sentiment_score),
min_score   = min(sentiment_score),
max_score   = max(sentiment_score),
.groups = "drop"
)
comparison_pos_neg
write.csv(comparison_table, "sentiment_group_summary.csv", row.names = FALSE)
write.csv(comparison_table,
"C:/Users/User/Desktop/sentiment_group_summary.csv",
row.names = FALSE)
library(dplyr)
set.seed(123)
sample_comments <- comments_clean %>%
sample_n(20)   # берём 20 случайных комментариев
head(sample_comments$comment_text, 3)
install.packages("httr")
install.packages("jsonlite")
library(httr)
library(jsonlite)
library(dplyr)
classify_comment_llm <- function(text) {
# короткий, чёткий промпт
prompt_text <- paste0(
"You are classifying public comments on a proposed CMS regulation (CMS-2023-0144).\n",
"Read the following comment and classify the stance toward the regulation.\n",
"Return exactly one of the following labels:\n",
"- Support\n",
"- Oppose\n",
"- Neutral\n\n",
"Comment:\n\"",
text,
"\"\n\n",
"Answer with only one word: Support, Oppose, or Neutral."
)
# ЗАМЕНИ На URL и модель своего провайдера LLM (пример для OpenAI)
res <- POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(
Authorization = paste("Bearer", "YOUR_API_KEY_HERE"),
"Content-Type" = "application/json"
),
body = toJSON(list(
model = "gpt-4.1-mini",  # или другую подходящую модель
messages = list(
list(role = "system", content = "You are a helpful assistant for text classification."),
list(role = "user",   content = prompt_text)
),
temperature = 0
), auto_unbox = TRUE)
)
# обработка результата
if (status_code(res) != 200) {
warning("API request failed with status: ", status_code(res))
return(NA_character_)
}
res_content <- content(res, as = "text", encoding = "UTF-8")
res_json    <- fromJSON(res_content)
raw_answer <- res_json$choices[[1]]$message$content
# почистим пробелы, переводы строк и обрежем до одного слова
clean_answer <- raw_answer %>%
stringr::str_trim() %>%
stringr::str_to_title()
# на всякий случай нормализуем
if (clean_answer %in% c("Support", "Oppose", "Neutral")) {
return(clean_answer)
} else {
return(NA_character_)
}
}
# добавим новую колонку llm_stance
sample_comments_llm <- sample_comments %>%
mutate(
llm_stance = sapply(comment_text, classify_comment_llm)
)
sample_comments_llm
write.csv(sample_comments_llm, "sample_comments_llm_stance.csv", row.names = FALSE)
sample_comments_llm
write.csv(sample_comments_llm, "sample_comments_llm_stance.csv", row.names = FALSE)
write.csv(
sample_comments_llm,
"C:/Users/User/Desktop/sample_comments_llm_stance.csv",
row.names = FALSE
)
#############################################
# 0. Setup
#############################################
# Install packages (run once if needed)
install.packages(c("readr", "dplyr", "stringr", "tidytext", "ggplot2", "httr", "jsonlite"))
library(quantregForest)
set.seed(42)
# Small but sufficient sample
idx <- sample(nrow(psa_all), 15000)
# 0) Check you are in the right folder (must show your .Rdata files)
getwd()
list.files(pattern = "\\.Rdata$", ignore.case = TRUE)
# 1) Load all PSA .Rdata files
rdata_files <- list.files(pattern = "^smpl_joint_.*50K\\.Rdata$", ignore.case = TRUE)
rdata_files
# 2) Read each file and extract the FIRST big matrix (PSA draws)
psa_list <- lapply(rdata_files, function(f) {
e <- new.env()
load(f, envir = e)
objs <- as.list(e)
mats <- objs[sapply(objs, is.matrix)]
if (length(mats) == 0) stop(paste("No matrix found in", f))
mats[[1]]
})
# 3) Bind into one big table
max_p <- max(sapply(psa_list, ncol))
# 1) Find PSA files
rdata_files <- list.files(pattern = "^smpl_joint_.*50K\\.Rdata$", ignore.case = TRUE)
rdata_files
# 2) Load each file and extract ONLY a 2D matrix
psa_list <- lapply(rdata_files, function(f) {
e <- new.env()
load(f, envir = e)
objs <- as.list(e)
# keep only true 2D objects
is_2d <- sapply(objs, function(x) {
d <- dim(x)
!is.null(d) && length(d) == 2
})
cand <- objs[is_2d]
if (length(cand) == 0) stop(paste("No 2D object found in", f))
# prefer a matrix; otherwise coerce the first 2D object to matrix
mat_idx <- which(sapply(cand, is.matrix))
x <- if (length(mat_idx) > 0) cand[[mat_idx[1]]] else cand[[1]]
as.matrix(x)
})
# 3) Now this will work
max_p <- max(sapply(psa_list, ncol))
# 0) Find PSA files
rdata_files <- list.files(pattern = "^smpl_joint_.*50K\\.Rdata$", ignore.case = TRUE)
print(rdata_files)
# 1) Load each file and extract a STRICT 2D matrix
psa_list <- lapply(rdata_files, function(f) {
e <- new.env()
load(f, envir = e)
objs <- as.list(e)
# keep only 2D objects
cand <- objs[sapply(objs, function(x) {
d <- dim(x)
!is.null(d) && length(d) == 2
})]
if (length(cand) == 0) stop(paste("No 2D object found in:", f))
m <- as.matrix(cand[[1]])  # force matrix no matter what
cat("Loaded:", f, "| dim:", paste(dim(m), collapse = "x"), "\n")
m
})
# 2) Compute max_p safely (NO sapply simplification issues)
ncols <- vapply(psa_list, function(m) ncol(m), integer(1))
max_p <- max(ncols)
common_names <- paste0("V", 1:max_p)
getwd()
list.files()
setwd("C:/Users/User/Desktop/Final_Project")
getwd()
list.files(pattern = "smpl_joint_.*50K\\.Rdata", ignore.case = TRUE)
rdata_files <- list.files(pattern = "^smpl_joint_.*50K\\.Rdata$", ignore.case = TRUE)
psa_list <- lapply(rdata_files, function(f) {
e <- new.env()
load(f, envir = e)
objs <- as.list(e)
mats <- objs[sapply(objs, is.matrix)]
mats[[1]]
})
max_p <- max(vapply(psa_list, ncol, integer(1)))
common_names <- paste0("V", 1:max_p)
psa_df_list <- lapply(psa_list, function(m) {
df <- as.data.frame(m)
colnames(df) <- paste0("V", seq_len(ncol(df)))
if (ncol(df) < max_p) for (j in (ncol(df)+1):max_p) df[[paste0("V", j)]] <- NA
df[, common_names, drop = FALSE]
})
psa_all <- do.call(rbind, psa_df_list)
WTP <- 50000
psa_all$NMB <- WTP * psa_all$V20 - psa_all$V18
cat("✅ psa_all restored | Rows:", nrow(psa_all), "Cols:", ncol(psa_all), "\n")
library(quantregForest)
set.seed(42)
# Sample for speed
idx <- sample(nrow(psa_all), 15000)
df_qrf <- psa_all[idx, ]
keep_cols <- paste0("V", 1:17)   # only clean predictors (no NA columns)
X <- df_qrf[, keep_cols, drop = FALSE]
y <- df_qrf$NMB
qrf <- quantregForest(x = X, y = y, ntree = 200, nodesize = 50, importance = TRUE)
pred_q10 <- predict(qrf, X, what = 0.10)
pred_q90 <- predict(qrf, X, what = 0.90)
print(summary(pred_q10))
print(summary(pred_q90))
imp_sorted <- sort(importance(qrf), decreasing = TRUE)
print(imp_sorted[1:10])
pred_q10 <- predict(qrf, X, what = 0.10)
pred_q90 <- predict(qrf, X, what = 0.90)
summary(pred_q10)
summary(pred_q90)
imp_sorted <- sort(importance(qrf), decreasing = TRUE)
imp_sorted[1:10]
names(imp_sorted) <- colnames(X)
imp_sorted[1:10]
# Создаём данные для гистограммы
df_predictions <- data.frame(
Scenario = rep(c("q10 (worst)", "q90 (best)"), each = length(pred_q10)),
NMB = c(pred_q10, pred_q90)
)
# Печатаем график
library(ggplot2)
ggplot(df_predictions, aes(x = NMB, fill = Scenario)) +
geom_histogram(binwidth = 500000, position = "dodge", alpha = 0.7) +
labs(title = "Distribution of NMB: Worst (q10) vs Best (q90) Cases", x = "Net Monetary Benefit (NMB)", y = "Frequency") +
scale_fill_manual(values = c("red", "green")) +
theme_minimal()
# Визуализируем топ-10 важнейших параметров
importance_data <- data.frame(
Variable = names(imp_sorted)[1:10],
Importance = imp_sorted[1:10]
)
# Строим bar plot
ggplot(importance_data, aes(x = reorder(Variable, Importance), y = Importance, fill = Importance)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top 10 Important Variables (QRF)", x = "Variable", y = "Importance") +
theme_minimal() +
scale_fill_viridis_c()
# Строим 2D plot для взаимодействия
library(pdp)
pdp_interaction <- partial(qrf, pred.var = c("V12", "V8"), train = df_qrf, grid.resolution = 20)
library(ggplot2)
# We only use clean predictors (V1..V17), so no NA
X_train <- X   # X used to train qrf (15k x 17)
# Grid for V12 and V8 (use 5%..95% to avoid extreme outliers)
v12_seq <- seq(quantile(X_train$V12, 0.05), quantile(X_train$V12, 0.95), length.out = 25)
v8_seq  <- seq(quantile(X_train$V8,  0.05), quantile(X_train$V8,  0.95), length.out = 25)
grid <- expand.grid(V12 = v12_seq, V8 = v8_seq)
# Base row = medians for all predictors
base <- as.data.frame(lapply(X_train, median))
# newdata: repeat base, then overwrite V12 and V8
newdata <- base[rep(1, nrow(grid)), ]
newdata$V12 <- grid$V12
newdata$V8  <- grid$V8
# Predict median (q50) NMB on this grid
grid$NMB_q50 <- as.numeric(predict(qrf, newdata, what = 0.50))
# Plot heatmap
p_heat <- ggplot(grid, aes(x = V12, y = V8, fill = NMB_q50)) +
geom_tile() +
labs(title = "QRF median NMB surface: V12 x V8",
x = "V12", y = "V8", fill = "Predicted NMB (q50)")
print(p_heat)
# Save to PDF
ggsave("QRF_heatmap_V12_V8.pdf", plot = p_heat, width = 8, height = 6)
savehistory("C:/Users/User/Desktop/Final_Project/console_history_raw.Rhistory")
